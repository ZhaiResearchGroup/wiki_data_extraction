{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import metapy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROCESSES=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wiki_old_updated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up error logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# modified from https://stackoverflow.com/questions/11232230/logging-to-two-files-with-different-settings\n",
    "def generate_logger(name):\n",
    "    handler = logging.FileHandler('{}.log'.format(name), mode='w')\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "loggers = [generate_logger('search/wiki_{}/wiki_{}'.format(i, i)) for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8650"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df['document'].tolist()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8650x994889 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18586133 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our raw queries using tf idf\n",
    "def transform_query(raw_query):\n",
    "    sparse_query = vectorizer.transform([raw_query])\n",
    "    return sparse_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the queries using MeTaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(summary):\n",
    "    return [sentence.strip() for sentence in summary.split(\".\")]\n",
    "\n",
    "def write_doc(sentences, filename, p_index):\n",
    "    # write document to file\n",
    "    with open(filename, 'w+') as doc_file:\n",
    "        for sentence in sentences:\n",
    "            doc_file.write(\"{}\\n\".format(sentence))\n",
    "    \n",
    "    # write metadata\n",
    "    with open('search/wiki_{}/metadata.dat'.format(p_index), 'w+') as meta_file:\n",
    "        for i in range(len(sentences)):\n",
    "            meta_file.write(\"SEN{}\\n\".format(i))\n",
    "\n",
    "def remove_old_idx(p_index):\n",
    "    call([\"rm\", \"-r\", \"search/idx_{}\".format(p_index)])\n",
    "    \n",
    "def get_stringified_list(idx, search_results):\n",
    "    return [idx.metadata(doc_id).get('content') for (doc_id, score) in search_results]\n",
    "\n",
    "# normalized = score / (num words in query) * log(num documents)\n",
    "def normalize_scores(search_results, sentences, summary):\n",
    "    num_words_in_query = len(summary.split(\" \"))\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    f = lambda score: score / (num_words_in_query * np.log10(num_sentences)) if (num_words_in_query * np.log10(num_sentences)) > 0 else np.nan\n",
    "    return [f(score) for (doc_id, score) in search_results]\n",
    "\n",
    "def search(document, summary, p_index, num_results=5):\n",
    "    sentences = [sentence.strip() for sentence in document.split(\".\")]\n",
    "    \n",
    "    # get rid of empty strings: https://stackoverflow.com/questions/3845423/remove-empty-strings-from-a-list-of-strings\n",
    "    sentences = list(filter(None, sentences))\n",
    "    \n",
    "    write_doc(sentences, 'search/wiki_{}/wiki_{}.dat'.format(p_index, p_index), p_index)\n",
    "    remove_old_idx(p_index)\n",
    "    \n",
    "    idx = metapy.index.make_inverted_index('search/config_{}.toml'.format(p_index))\n",
    "    \n",
    "    ranker = metapy.index.OkapiBM25()\n",
    "    \n",
    "    query = metapy.index.Document()\n",
    "    query.content(summary)\n",
    "    \n",
    "    search_results = ranker.score(idx, query, num_results=num_results)\n",
    "\n",
    "    normalized_scores = normalize_scores(search_results, sentences, summary)\n",
    "    \n",
    "    stringified_results = get_stringified_list(idx, search_results)\n",
    "\n",
    "    return list(zip(stringified_results, normalized_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate raw and transform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(args):\n",
    "    p_df, p_index = args\n",
    "    \n",
    "    logger = loggers[p_index - 1]\n",
    "\n",
    "    data = {\"title\": [], \"raw_query\": [], \"sentence_summary\": [], \"document\": [], \"normalized_score\": []}\n",
    "    \n",
    "    queries = []\n",
    "\n",
    "    documents_generated = 0\n",
    "    queries_generated = 0\n",
    "\n",
    "    total_documents = p_df.shape[0]\n",
    "\n",
    "    for row in p_df.iterrows():\n",
    "        title = row[1]['title']\n",
    "        summary = row[1]['summary']\n",
    "        document = row[1]['document']\n",
    "        headers = row[1]['headers']\n",
    "        sidebar = row[1]['sidebar']\n",
    "        \n",
    "        logger.debug(title)\n",
    "        \n",
    "        if (not document.strip()) or (not summary.strip()):\n",
    "            continue\n",
    "        \n",
    "        sentences = get_sentences(summary)\n",
    "        sentences = list(filter(None, sentences))\n",
    "        \n",
    "        # only take first 3 sentences\n",
    "        sentences = sentences[:3]\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            # extract query\n",
    "            raw_queries = search(document, sentence, p_index)\n",
    "            \n",
    "            for raw_query, normalized_score in raw_queries:\n",
    "                query = transform_query(raw_query)\n",
    "\n",
    "                # add query info to data\n",
    "                data[\"raw_query\"].append(raw_query)\n",
    "                data[\"sentence_summary\"].append(sentence)\n",
    "                data[\"title\"].append(title)\n",
    "                data[\"document\"].append(document)\n",
    "                data[\"normalized_score\"].append(normalized_score * (-1)) # TODO: remove -1. This is just to differentiate\n",
    "\n",
    "                queries.append(query)\n",
    "            \n",
    "                queries_generated += 1\n",
    "            \n",
    "        # more experimental queries\n",
    "        if not pd.isnull(headers):\n",
    "            for header in headers.split(\" --- \"):\n",
    "                sentence_summary = search(summary, header, p_index, num_results=1)\n",
    "                if len(sentence_summary) == 0:\n",
    "                    continue\n",
    "                \n",
    "                header_query = transform_query(header)\n",
    "\n",
    "                data[\"raw_query\"].append(header)\n",
    "                data[\"sentence_summary\"].append(sentence_summary[0][0])\n",
    "                data[\"title\"].append(title)\n",
    "                data[\"document\"].append(document)\n",
    "                data[\"normalized_score\"].append(sentence_summary[0][1])\n",
    "\n",
    "                queries.append(header_query)\n",
    "\n",
    "                queries_generated += 1\n",
    "\n",
    "        if not pd.isnull(sidebar):\n",
    "            for sidebar_entry in sidebar.split(\" --- \"):\n",
    "                sentence_summary = search(summary, sidebar_entry, p_index, num_results=1)\n",
    "                if len(sentence_summary) == 0:\n",
    "                    continue\n",
    "                \n",
    "                sidebar_query = transform_query(sidebar_entry)\n",
    "\n",
    "                data[\"raw_query\"].append(sidebar_entry)\n",
    "                data[\"sentence_summary\"].append(sentence_summary[0][0])\n",
    "                data[\"title\"].append(title)\n",
    "                data[\"document\"].append(document)\n",
    "                data[\"normalized_score\"].append(sentence_summary[0][1])\n",
    "\n",
    "                queries.append(sidebar_query)\n",
    "\n",
    "                queries_generated += 1\n",
    "        \n",
    "        documents_generated += 1\n",
    "        \n",
    "        if documents_generated % 20 == 0:\n",
    "            print(\"Process {}: Generated {} queries for {} documents, {:.4f}% complete\".format(p_index, queries_generated, documents_generated, documents_generated/total_documents * 100))\n",
    "    \n",
    "    print(\"Process {}: Finished generating queries\".format(p_index))\n",
    "\n",
    "    return pd.DataFrame(data=data), sp.sparse.vstack(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_queries_data(df_queries, queries):\n",
    "    # reorganize dataframe\n",
    "    df_final = df_queries[['title', 'raw_query', 'normalized_score', 'sentence_summary', 'document']]\n",
    "    df_final = df_final.reindex(df_final.index.rename('query_index'))\n",
    "    df_final.index = df_final.index.astype(int)\n",
    "    \n",
    "    # store queries matrix\n",
    "    sp.sparse.save_npz(\"out/queries_matrix.npz\", queries)\n",
    "\n",
    "    # store queries csv\n",
    "    df_final.to_csv(\"out/wiki_queries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 5: Finished generating queries\n",
      "Finished generating queries\n"
     ]
    }
   ],
   "source": [
    "def generate_queries_multiprocess(df, num_processes=NUM_PROCESSES):  \n",
    "    pool = Pool(processes=num_processes)\n",
    "    \n",
    "    p_dfs = np.array_split(df, num_processes)\n",
    "    \n",
    "    args_by_process = [(p_dfs[i], i+1) for i in range(len(p_dfs))]\n",
    "    results = pool.map(generate_queries, args_by_process)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    df_queries = pd.concat([result[0] for result in results], ignore_index=True)\n",
    "    queries = sp.sparse.vstack([result[1] for result in results])\n",
    "    \n",
    "    return df_queries, queries\n",
    "\n",
    "def create_queries():\n",
    "    print(\"Started generating queries\")\n",
    "\n",
    "    df_queries, queries = generate_queries_multiprocess(df[:16])\n",
    "\n",
    "    store_queries_data(df_queries, queries)\n",
    "\n",
    "    print(\"Finished generating queries\")\n",
    "\n",
    "create_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store vectorizer\n",
    "pickle_out = open(\"out/vectorizer.pickle\",\"wb\")\n",
    "pickle.dump(vectorizer, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['James Bond.txt',\n",
       " 'Speed.txt',\n",
       " 'Official language.txt',\n",
       " 'Federal Register.txt',\n",
       " 'Flash memory.txt',\n",
       " 'Counties of Sweden.txt',\n",
       " '1980 United States Census.txt',\n",
       " 'Percussion instrument.txt',\n",
       " 'South Australia.txt',\n",
       " 'Colorado.txt',\n",
       " 'Anglosphere.txt',\n",
       " 'Tornado.txt',\n",
       " 'Channel 5 (UK).txt',\n",
       " 'Nordic countries.txt',\n",
       " 'Nasal consonant.txt',\n",
       " 'WebCite.txt']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in df[\"title\"][0:16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_df = pd.read_csv('out/wiki_queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_info(print_df):\n",
    "#     for row in print_df.iterrows():\n",
    "#         print(\"----- QUERY -----\")\n",
    "#         print(row[1]['raw_query'], row[1]['normalized_score'])\n",
    "#         print(\"\\n\")\n",
    "#         print(\"----- SUMMARY -----\")\n",
    "#         print(row[1]['sentence_summary'])\n",
    "#         print(\"\\n--------------------\\n\")\n",
    "\n",
    "# print_info(out_df[15:46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_info(out_df.loc[out_df.raw_query.str.len() < .5 * out_df.sentence_summary.str.len(), ['raw_query', 'sentence_summary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEAS:\n",
    "# - pull stuff from the right-hand side as queries (ex/ \"Created by\")\n",
    "# - pull content headers as queries (ex/ \"origins\")\n",
    "# - use the title as another query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
