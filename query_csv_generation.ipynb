{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import metapy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROCESSES=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up error logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# modified from https://stackoverflow.com/questions/11232230/logging-to-two-files-with-different-settings\n",
    "def generate_logger(name):\n",
    "    handler = logging.FileHandler('{}.log'.format(name), mode='w')\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "loggers = [generate_logger('search/wiki_{}/wiki_{}'.format(i, i)) for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in our vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open(\"out/vectorizer.pickle\",\"rb\")\n",
    "vectorizer = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our raw queries using tf idf\n",
    "def transform_query(raw_query):\n",
    "    combined_query = \". \".join(raw_query) + \".\"\n",
    "    sparse_query = vectorizer.transform([combined_query])\n",
    "    return sparse_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the queries using MeTaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(paragraph):\n",
    "    return [sentence.strip() for sentence in paragraph.split(\".\")]\n",
    "\n",
    "def write_doc(document, filename, p_index):\n",
    "    sentences = [get_sentences(paragraph) for paragraph in document.split(\"\\n\")]\n",
    "    \n",
    "    # flatten list\n",
    "    sentences = list(itertools.chain.from_iterable(sentences))\n",
    "    \n",
    "    # get rid of empty strings: https://stackoverflow.com/questions/3845423/remove-empty-strings-from-a-list-of-strings\n",
    "    sentences = list(filter(None, sentences))\n",
    "\n",
    "    # write document to file\n",
    "    with open(filename, 'w+') as doc_file:\n",
    "        for sentence in sentences:\n",
    "            doc_file.write(\"{}\\n\".format(sentence))\n",
    "    \n",
    "    # write metadata\n",
    "    with open('search/wiki_{}/metadata.dat'.format(p_index), 'w+') as meta_file:\n",
    "        for i in range(len(sentences)):\n",
    "            meta_file.write(\"SEN{}\\n\".format(i))\n",
    "\n",
    "def remove_old_idx(p_index):\n",
    "    call([\"rm\", \"-r\", \"search/idx_{}\".format(p_index)])\n",
    "    \n",
    "def get_stringified_list(idx, search_results):\n",
    "    return [idx.metadata(doc_id).get('content') for (doc_id, score) in search_results]\n",
    "\n",
    "def search(document, summary, p_index):\n",
    "    write_doc(document, 'search/wiki_{}/wiki_{}.dat'.format(p_index, p_index), p_index)\n",
    "    remove_old_idx(p_index)\n",
    "    \n",
    "    idx = metapy.index.make_inverted_index('search/config_{}.toml'.format(p_index))\n",
    "    \n",
    "    ranker = metapy.index.OkapiBM25()\n",
    "    \n",
    "    query = metapy.index.Document()\n",
    "    query.content(summary)\n",
    "    \n",
    "    search_results = ranker.score(idx, query, num_results=5)\n",
    "    return get_stringified_list(idx, search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate raw and transform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(args):\n",
    "    p_df, p_index = args\n",
    "    \n",
    "    logger = loggers[p_index - 1]\n",
    "\n",
    "    data = {\"title\": [], \"raw_query\": [], \"sentence_summary\": []}\n",
    "    \n",
    "    queries = []\n",
    "\n",
    "    documents_generated = 0\n",
    "    queries_generated = 0\n",
    "\n",
    "    total_documents = p_df.shape[0]\n",
    "\n",
    "    for row in p_df.iterrows():\n",
    "        title = row[1]['title']\n",
    "        summary = row[1]['summary']\n",
    "        document = row[1]['document']\n",
    "        \n",
    "        logger.debug(title)\n",
    "        \n",
    "        if (not document.strip()) or (not summary.strip()):\n",
    "            continue\n",
    "        \n",
    "        sentences = get_sentences(summary)\n",
    "        sentences = list(filter(None, sentences))\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            # extract query\n",
    "            raw_query = search(document, sentence, p_index)\n",
    "            query = transform_query(raw_query)\n",
    "\n",
    "            # add query info to data\n",
    "            data[\"raw_query\"].append(raw_query)\n",
    "            data[\"sentence_summary\"].append(sentence)\n",
    "            data[\"title\"].append(title)\n",
    "\n",
    "            queries.append(query)\n",
    "            \n",
    "            queries_generated += 1\n",
    "        \n",
    "        documents_generated += 1\n",
    "        \n",
    "        if documents_generated % 400 == 0:\n",
    "            print(\"Process {}: Generated {} queries for {} documents, {:.4f}% complete\".format(p_index, queries_generated, documents_generated, documents_generated/total_documents * 100))\n",
    "    \n",
    "    print(\"Process {}: Finished generating queries\".format(p_index))\n",
    "\n",
    "    return pd.DataFrame(data=data), sp.sparse.vstack(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_queries_data(df_queries, queries, index):\n",
    "    # reorganize dataframe\n",
    "    df_final = df_queries[['title', 'raw_query', 'sentence_summary']]\n",
    "    df_final = df_final.reindex(df_final.index.rename('query_index'))\n",
    "    df_final.index = df_final.index.astype(int)\n",
    "    \n",
    "    # store queries matrix\n",
    "    sp.sparse.save_npz(\"out/queries_matrix_{}.npz\".format(index), queries)\n",
    "\n",
    "    # store queries csv\n",
    "    df_final.to_csv(\"out/wiki_queries_{}.csv\".format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Started generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Batch 1: Finished generating queries\n",
      "Batch 2: Started generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Batch 2: Finished generating queries\n",
      "Batch 3: Started generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Batch 3: Finished generating queries\n",
      "Batch 4: Started generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Batch 4: Finished generating queries\n",
      "Batch 5: Started generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Batch 5: Finished generating queries\n",
      "Batch 6: Started generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 5: Finished generating queries\n",
      "Batch 6: Finished generating queries\n",
      "Batch 7: Started generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Batch 7: Finished generating queries\n",
      "Batch 8: Started generating queries\n",
      "Process 5: Finished generating queries\n",
      "Process 6: Finished generating queries\n",
      "Process 4: Finished generating queries\n",
      "Process 2: Finished generating queries\n",
      "Process 7: Finished generating queries\n",
      "Process 1: Finished generating queries\n",
      "Process 8: Finished generating queries\n",
      "Process 3: Finished generating queries\n",
      "Batch 8: Finished generating queries\n"
     ]
    }
   ],
   "source": [
    "def generate_queries_multiprocess(df, batch_index, num_processes=NUM_PROCESSES):  \n",
    "    pool = Pool(processes=num_processes)\n",
    "    \n",
    "    p_dfs = np.array_split(df, num_processes)\n",
    "    \n",
    "    args_by_process = [(p_dfs[i], i+1) for i in range(len(p_dfs))]\n",
    "    results = pool.map(generate_queries, args_by_process)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    df_queries = pd.concat([result[0] for result in results], ignore_index=True)\n",
    "    queries = sp.sparse.vstack([result[1] for result in results])\n",
    "    \n",
    "    return df_queries, queries\n",
    "\n",
    "def create_queries_batched(num_batches=8):\n",
    "    for i in range(1, 9):\n",
    "        batch_df = pd.read_csv(\"data/wiki_summaries_{}.csv\".format(i))\n",
    "        \n",
    "        print(\"Batch {}: Started generating queries\".format(i))\n",
    "        \n",
    "        df_queries, queries = generate_queries_multiprocess(batch_df, i)\n",
    "        \n",
    "        store_queries_data(df_queries, queries, i)\n",
    "        \n",
    "        print(\"Batch {}: Finished generating queries\".format(i))\n",
    "\n",
    "create_queries_batched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
